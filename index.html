<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='Diffusion2GAN for One-step Text-to-Image Synthesis. arXiv2024'/>
    <meta property='og:url' content='https://mingukkang.github.io/Diffusion2GAN/'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingukkang.github.io/">Arman Zarei</a><sup>*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://richzhang.github.io/">Keivan Rezaei</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.connellybarnes.com/work/">Samyadeep Basu</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://research.adobe.com/person/sylvain-paris/">Mehrdad Saberi</a>,
            </span>
            <span class="author-block">
              <a href="https://suhakwak.github.io/">Mazda Moayeri</a>,
            </span>
            <span class="author-block">
              <a href="https://jaesik.info/">Priyatham Kattakinda</a>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/eli-shechtman/">Soheil Feizi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Department of Computer Science, University of Maryland</span>
          </div>

          <div class="is-size-5 publication-venue">
            arXiv Preprint
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ArmanZarei/Mitigating-T2I-Comp-Issues"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image diffusion-based generative models have the stunning ability to generate photo-realistic images and achieve state-of-the-art low FID scores on challenging image generation benchmarks.
            However, one of the primary failure modes of these text-to-image generative models is in composing attributes, objects, and their associated relationships accurately into an image.
            In our paper, we investigate this compositionality-based failure mode and highlight that imperfect text conditioning with CLIP text-encoder is one of the primary reasons behind the inability of these models to generate high-fidelity compositional scenes.
            In particular, we show that (i) there exists an optimal text-embedding space that can generate highly coherent compositional scenes showing that the output space of the CLIP text-encoder is sub-optimal,
            and (ii) the final token embeddings in CLIP are erroneous as they often include attention contributions from unrelated tokens in compositional prompts.
            Our main finding shows that the best compositional improvements can be achieved (without harming the model's FID score) by fine-tuning <i>it only</i> a simple and parameter-efficient linear projection on CLIP's representation space in Stable-Diffusion variants using a small set of compositional image-text pairs.
            This result demonstrates that the sub-optimality of the CLIP's output space is a major error source.
            We also show that re-weighting the erroneous attention contributions in CLIP can lead to slightly improved compositional performances.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><tt>WiCLP</tt>: Window-Based Compositional Linear Projection</h2>

        <!-- Prompt Interpolation image -->
        
        <div class="content has-text-centered">
            <img src="./static/images/main_figure.png" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            we propose <b><u>Wi</u></b>ndow-based <b><u>C</u></b>ompositional <b><u>L</u></b>inear <b><u>P</u></b>rojection (<tt>WiCLP</tt>),
            a <i>lightweight</i> fine-tuning method that significantly improves the model’s performance on compositional prompts, yielding results comparable to existing baselines.
            <tt>WiCLP</tt> obtains new embeddings for tokens in the input prompt by applying a linear projection on tokens in conjunction with a set of their adjacent ones,
            i.e., tokens within a specified window.
            This method uses linear projection and contextual cues from neighboring tokens to refine CLIP text encoder embeddings,
            producing embeddings that more effectively capture the compositional scene.
          </p>
          <br>
        </div>
        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">(i) <tt>WiCLP</tt> improves Compositionality!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/Diversity_d2g0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning2.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        
        <div class="content has-text-justified">
          <p>
            [TODO] -:?
          </p>
          <br>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">(ii) <tt>WiCLP</tt> improves Cross-Attention Masks!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/Diversity_d2g0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning2.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            [TODO] :-?
          </p>
          <br>
        </div>

        <h3 class="title is-4">(iii) <span style="font-variant: small-caps;">Switch-Off</span> enables <tt>WiCLP</tt> to preserve Model Utility</h3>

      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Possible Sources of Compositional Issues</h2>
        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Errorneous Attention Contributions in CLIP</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/LatentLPIPS_recon.png" width="100%">
        </div>
        <div class="content has-text-justified">
          <p>
            We conduct an image reconstruction experiment by directly optimizing a single latent with different loss functions. Reconstruction with <b>LPIPS</b> roughly reproduces the target image, but at the cost of needing to decode into pixels. <b>LatentLPIPS</b> alone cannot precisely reconstruct the image. However, our ensembled augmentation, <b>E-LatentLPIPS</b>, can more precisely reconstruct the target while operating directly in the latent space. 
          </p>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Sub-Optimality of CLIP Text-Encoder</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/Radar_chart_sdxl.png" width="80%">
        </div>
        <div class="content has-text-justified">
          <p>
            We compare <b>Diffusion2GAN</b> with two concurrent one-step diffusion distillation generators: <b>SDXL-turbo</b> and <b>SDXL-Lightning</b>, as well as their teacher model, <b>SDXL-Base-1.0</b>. Although <b>SDXL-Lightning</b> produces more diverse images compared to <b>Diffusion2GAN</b> and <b>SDXL-Turbo</b>, it does so at the expense of FID and CLIP-score. On the other hand, <b>Diffusion2GAN</b> demonstrates better FID and CLIP-score than <b>SDXL-Turbo</b> and <b>SDXL-Lightning</b> but generates more diverse images compared to <b>SDXL-Turbo</b>.
          </p>
          <br>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4"><tt>WiCLP</tt> Achieves Superior Compositional Performance on Stable Diffusion Models in T2I-CompBench</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/COCO2014_benchmark.png" width="70%">
        </div>
        <div class="content has-text-justified">
        </div>

      </div>
    </div>
  </div>
    
    <br>
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
      <!-- Latent space editing applications -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related Works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
            </p>
          </div>
          <!-- Prompt Interpolation image -->
        </div>
      </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO’s Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@article{zarei2024understanding,
      title={Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models},
      author={Zarei, Arman and Rezaei, Keivan and Basu, Samyadeep and Saberi, Mehrdad and Moayeri, Mazda and Kattakinda, Priyatham and Feizi, Soheil},
      journal={arXiv preprint arXiv:2406.07844},
      year={2024}
}</code></pre>
  </div>
</section>



<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/corgi_input.jpeg", "label": "Generated by SDXL-Diffusion2GAN (512px)",},
  {"src": "./static/images/corgi_output.jpeg", "label": "8 x Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Generated by Diffusion2GAN (512px)",
  };
  let outputImage = {
    label: "8 x Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
