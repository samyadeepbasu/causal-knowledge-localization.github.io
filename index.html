<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='Diffusion2GAN for One-step Text-to-Image Synthesis. arXiv2024'/>
    <meta property='og:url' content='https://mingukkang.github.io/Diffusion2GAN/'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Mitigating Compositional Issues in Text-to-Image Generative Models via Enhanced Text Embeddings</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingukkang.github.io/">Arman Zarei</a><sup>*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://richzhang.github.io/">Keivan Rezaei</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.connellybarnes.com/work/">Samyadeep Basu</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://research.adobe.com/person/sylvain-paris/">Mehrdad Saberi</a>,
            </span>
            <span class="author-block">
              <a href="https://suhakwak.github.io/">Mazda Moayeri</a>,
            </span>
            <span class="author-block">
              <a href="https://jaesik.info/">Priyatham Kattakinda</a>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/eli-shechtman/">Soheil Feizi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Department of Computer Science, University of Maryland</span>
          </div>

          <div class="is-size-5 publication-venue">
            arXiv Preprint
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.07844"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ArmanZarei/Mitigating-T2I-Comp-Issues"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image diffusion-based generative models have the stunning ability to generate photo-realistic images and achieve state-of-the-art low FID scores on challenging image generation benchmarks.
            However, one of the primary failure modes of these text-to-image generative models is in composing attributes, objects, and their associated relationships accurately into an image.
            In our paper, we investigate this compositionality-based failure mode and highlight that imperfect text conditioning with CLIP text-encoder is one of the primary reasons behind the inability of these models to generate high-fidelity compositional scenes.
            In particular, we show that (i) there exists an optimal text-embedding space that can generate highly coherent compositional scenes showing that the output space of the CLIP text-encoder is sub-optimal,
            and (ii) the final token embeddings in CLIP are erroneous as they often include attention contributions from unrelated tokens in compositional prompts.
            Our main finding shows that the best compositional improvements can be achieved (without harming the model's FID score) by fine-tuning <i>it only</i> a simple and parameter-efficient linear projection on CLIP's representation space in Stable-Diffusion variants using a small set of compositional image-text pairs.
            This result demonstrates that the sub-optimality of the CLIP's output space is a major error source.
            We also show that re-weighting the erroneous attention contributions in CLIP can lead to slightly improved compositional performances.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered"><tt>WiCLP</tt>: Window-Based Compositional Linear Projection</h2>

        <!-- Prompt Interpolation image -->
        
        <div class="content has-text-centered">
            <img src="./static/images/main_figure.png" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            we propose <b><u>Wi</u></b>ndow-based <b><u>C</u></b>ompositional <b><u>L</u></b>inear <b><u>P</u></b>rojection (<tt>WiCLP</tt>),
            a <i>lightweight</i> fine-tuning method that significantly improves the model’s performance on compositional prompts, yielding results comparable to existing baselines.
            <tt>WiCLP</tt> obtains new embeddings for tokens in the input prompt by applying a linear projection on tokens in conjunction with a set of their adjacent ones,
            i.e., tokens within a specified window.
            This method uses linear projection and contextual cues from neighboring tokens to refine CLIP text encoder embeddings,
            producing embeddings that more effectively capture the compositional scene.
          </p>
          <br>
        </div>
        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">(i) <tt>WiCLP</tt> improves Compositionality!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a yellow book and a red vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red apple and a green train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a green leaf and a yellow butterfly.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A black and white cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red book.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/A bathroom with green tile and a red shower curtain.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bench and a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue backpack and a red chair.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a blue bowl and a red train.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a brown book and a red sheep.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a fluffy towel and a glass cup.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a plastic container and a fluffy teddy bear.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red chair and a gold clock.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a red pen and a blue notebook.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a round cookie and a square container.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/a wooden floor and a fluffy rug.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/The leather jacket and fluffy scarf keep the cold at bay.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/prompt_visualizations/wooden pencil and a glass plate.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        
        <!-- <div class="content has-text-justified">
          <p>
            [TODO] -:?
          </p>
          <br>
        </div> -->
        <br><br>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">(ii) <tt>WiCLP</tt> improves Cross-Attention Masks!</h3>
        
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a blue backpack and a red bench.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green bench and a yellow dog.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a red book and a yellow vase.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a bathroom has brown wall and gold counters.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a black cat sitting in a green bowl.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a brown boat and a blue cat.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/attention_visualizations/a green blanket and a blue pillow.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>

        <!-- <div class="content has-text-justified">
          <p>
            [TODO] :-?
          </p>
          <br>
        </div> -->
        <br><br>

        <h3 class="title is-4">(iii) <span style="font-variant: small-caps;">Switch-Off</span> enables <tt>WiCLP</tt> to preserve Model Utility</h3>
        <div class="container is-max-desktop has-text-centered">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-t2i1">
              <img id="myt2i0" src="./static/images/Diversity_d2g0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning0.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning1.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_d2g2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_turbo2.png"
              class="interpolation-image"/>
            </div>
            <div class="item item-t2i1">
              <img id="myt2i1" src="./static/images/Diversity_lightning2.png"
              class="interpolation-image"/>
            </div>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Fine-tuning models or adding modules to a base model often results in a degradation of image quality and an increase in the Fréchet Inception Distance (FID) score.
            To balance the trade-off between improved compositionality and the quality of generated images for clean prompts --an important issue in existing work-- inspired by Hertz et al [1],
            we adopt <span style="font-variant: small-caps;">Switch-Off</span>, where we apply the linear projection only during the initial steps of inference.
            More precisely, given a time-step threshold &tau;, for <i>t </i> &geq; &tau;,
            we use <tt>WiCLP</tt>, while for for <i>t </i> < &tau;,
            we use the unchanged embedding (output of CLIP text encoder) as the input to the cross-attention layers.
          </p>
          <br>
        </div>

        <br><br>
      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Possible Sources of Compositional Issues</h2>
        <div class="content has-text-justified">
          <p>
            We investigate two possible sources of compositionality issues in text-to-image models.
          </p>
        </div>
        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Source (i): Errorneous Attention Contributions in CLIP Text Encoder</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/attn_cont/attn_cont.jpg" width="100%">
        </div>
        <div class="content has-text-justified">
          <p>
            Above Figure visualizes the attention contribution of both T5 and CLIP text-encoder in the last layer &ell; = 11
            for the prompt "a green bench and a red car".
            Ideally, the attention mechanism should guide the token "car" to focus more on "red" than "green",
            but in the last layer of the CLIP text-encoder, "car" significantly attends to "green".
            In contrast, T5 shows a more consistent attention pattern, with "red" contributing more to the token "car" and "green" contributing more to the token "bench".
          </p>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4">Source (ii): Sub-Optimality of CLIP Text-Encoder</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/opt_text_embedding/opt_text_embedding.png" width="80%">
        </div>
        <div class="content has-text-justified">
          <p>
            we observe that the UNet is capable of generating compositional scenes if propoer text-embeddings is given as the input.
            This proper embedding is obtained by freezing the UNet and optimizing diffusion loss over images with high VQA scores,
            using the text embedding as the optimization variable.
            We consistently improve VQA scores across a variety of compositional prompts (i.e., color, texture, and shape).
            This indicates that CLIP text-encoder does not output the proper text-embedding suitable for generating compositional scenes.
            However, that optimized embedding space exists, highlighting the ability of UNet to generate coherent compositional scenes when a proper text-embedding is given.
          </p>
          <br>
        </div>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4"><tt>WiCLP</tt> Achieves Superior Compositional Performance on Stable Diffusion Models in T2I-CompBench!</h3>
        
        <div class="content has-text-centered">
            <img src="./static/images/experiments/results.png" width="90%">
        </div>
        <div class="content has-text-justified">
          <p>
            VQA scores of our method and other discussed baselines are provided in above Table.
            As shown, <tt>WiCLP</tt> significantly improves upon the baselines and
            achieves higher VQA scores compared to other state-of-the-art methods, despite its simplicity.
          </p>
          <br>
        </div>
      </div>
    </div>
  </div>
    
    <br>
    <!--/ Matting. -->
    <!-- <div class="container is-max-desktop">
    
      Latent space editing applications
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related Works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
            </p>
          </div>
          Prompt Interpolation image
        </div>
      </div>

    Concurrent Work.
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This project was supported in part by a grant from an NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271, ARO’s Early Career Program Award 310902-00001, Army Grant No. W911NF2120076, the NSF award CCF2212458, NSF Award No. 2229885 (NSF Institute for Trustworthy AI in Law and Society, TRAILS), an Amazon Research Award and an award from Capital One.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@article{zarei2024understanding,
      title={Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models},
      author={Zarei, Arman and Rezaei, Keivan and Basu, Samyadeep and Saberi, Mehrdad and Moayeri, Mazda and Kattakinda, Priyatham and Feizi, Soheil},
      journal={arXiv preprint arXiv:2406.07844},
      year={2024}
}</code></pre>
  </div>
</section>



<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/corgi_input.jpeg", "label": "Generated by SDXL-Diffusion2GAN (512px)",},
  {"src": "./static/images/corgi_output.jpeg", "label": "8 x Upsampled by GigaGAN (4K)",}
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Generated by Diffusion2GAN (512px)",
  };
  let outputImage = {
    label: "8 x Upsampled by GigaGAN (4K)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  let images = [inputImage, outputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
